\documentclass{article}
\usepackage{mathtools}
\usepackage{mathrsfs}
\newcommand{\vect}[1]{\boldsymbol{#1}}

\title{Monte Carlo Integrator}
\date{August 19, 2018}
\author{Ben Champion \\ bwc3252@rit.edu}
\begin{document}
\maketitle

\section{Basic Algorithm}

The integrand $f(x)$ is sampled over a domain $V$ according to a distribution
$p(x)$. The Monte Carlo integral over $V$ is
\begin{equation}
    I = {{1 \over N} \sum_{n=0}^{N} {f(x_n) \over {p(x_n)}}}.
\end{equation}
The variance estimate for this integral is given by
\begin{equation}
    \sigma^2 = {1 \over N^2}{\sum\limits_{n=0}^{N} {(g(x_n) - \langle g \rangle)}^2},
\end{equation}
where
\begin{equation}
    g(x) = {f(x) \over {p(x)}}
\end{equation}
and $\langle g \rangle$ is the average of $g(x)$ over $n$. \\

In order to reduce this variance without increasing the number of points
sampled, the distribution $p(x)$ is chosen to approximate $f(x)$. This is done
by iteratively sampling from a Gaussian Mixture Model (GMM), integrating using
these samples, and retraining the GMM to better approximate the integrand,
stopping either at a predetermined number of iterations or when the variance
falls below a certain threshold. When training the GMM each sample is weighted
by the ratio of its function value to its responsibility under the current
distribution, or
\begin{equation}
    w(x) = \left| {f(x) \over p(x)} \right|.
\end{equation}
Weighting the samples in this way means that the integrator will converge to
the correct distribution regardless of how the first iteration is sampled. In
practice, the first iteration is usually sampled uniformly, although the user
can provide samples and responsibilities from any prior distribution for the
first iteration.

Each iteration, the integral result and variance are updated with a running
average. The total integral is
\begin{equation}
    I_{total} = {{M I_{prev} + I} \over {M + 1}},
\end{equation}
where $M$ is the number of previous iterations. Note that this weighting scheme
assumes that the same number of samples $n$ is used for each iteration. The
updated variance is
\begin{equation}
    \sigma^2_{total} = {{M \sigma^2_{prev} + \sigma^2} \over {M + 1}}.
\end{equation}

\section{Initialization}

Whether using \texttt{monte\_carlo\_integrator.py} directly or within
\texttt{mcsampler\_new.py}, the integrator is initialized with the following
parameters:

\begin{enumerate}

\item \texttt{d}: Number of dimensions of the integrand's domain

\item \texttt{bounds}: \texttt{(d $\times$ 2)} array where each row is
\texttt{[left\_limit, right\_limit]} for its corresponding dimension

\item \texttt{gmm\_dict}: Python dictionary where each key is a tuple of
dimensions and each value is either a GMM object or \texttt{None}

\item \texttt{n\_comp}: Number of Gaussian components per mixture model

\end{enumerate}
The integrator is called using \texttt{monte\_carlo\_integrator.integrate(func)},
where \texttt{func} is a numpy vectorized function. The \texttt{var\_thresh},
\texttt{min\_iter}, and \texttt{max\_iter} parameters can also be specified when
calling the integrator.

\section{Truncated Multivariate Gaussian Samples}

The file \texttt{multivariate\_truncnorm.py} provides a \texttt{sample()}
function to draw samples from a multivariate normal distribution inside a set of
d-dimensional rectangular bounds. The parameters of \texttt{sample()} are the
mean and covariance of the distribution, the bounds of the sampling region, and
the number of samples to draw.

\subsection{Standard Multivariate Gaussian Sampling}

In general, multivariate Gaussian samples can be generated by sampling from a
Gaussian that is centered at the origin with a covariance of 1, applying a
linear transformation determined from the covariance matrix, and shifting the
resulting samples so that the mean falls in the correct place. For a covariance
matrix $\Sigma$ and mean $\mu$, the final samples are generated as
\begin{equation}
    x' = \lambda^{1/2} \phi x + \mu,
\end{equation}
where $x'$ is the final sample, $\lambda$ is diagonal matrix of the eigenvalues
of $\Sigma$, $\phi$ is a matrix of the eigenvectors of $\Sigma$, and $x$ is the
original "base sample." \\

\subsection{Fast Modified Rejection Sampling}

Truncated samples are generated through a similar process. Scipy's
\texttt{truncnorm} can be used to generate univariate truncated Gaussian
samples. Univariate samples with covariance 1 and mean 0 are used to create an
array of multivariate samples centered at the origin with covariance 1. The
above transformation is used to create truncated samples from the
desired distribution. \\

The problem with this approach, however, is that the bounds are transformed
along with the points. This means that the samples fill a d-dimensional
parallelogram-shaped region rather than a d-dimensional rectangular one. This
problem is remedied by drawing the initial samples from a slightly larger region
than is required and rejecting the samples that fall outside the bounds after
being transformed. \\

These new bounds are determined using the inverse of the transformation, or
\begin{equation}
    r = [\lambda^{1/2} \phi]^{-1}.
\end{equation}

Each corner point of the desired sampling region is transformed by $r$ to find
the new, transformed corner points.  The minimum and maximum coordinates for
each dimension describe the smallest rectangular region that, when transformed,
will contain the desired region. \\

Unlike traditional rejection sampling, where points are sampled from the entire
distribution and then truncated in the end, this method will not suffer drops in
efficiency based on where the mean of the distribution is in relation to the
bounds. Instead, the limiting factor is the geometry of the transformation.

\section{Gaussian Mixture Model}

\subsection{Basic Gaussian Mixture Model Implementation}

The Gaussian mixture model implemented in \texttt{gaussian\_mixture\_model.py}
fits a mixture of $k$ Gaussians to a set of weighted data using the
expectation-maximization (EM) algorithm. \\

The EM algorithm fits the $k$ Gaussians, described by $\mu_k$ (the means),
$\Sigma_k$ (the covariances), and $\pi_k$ (the weights), such that the likelihood
of the model $\mathscr{L}$ is maximized. For data points $x$ and sample weights
$w$,
\begin{equation}
    \mathscr{L} = \prod_n P(x_n).
\end{equation}
$P(x)$ is the probability of each point $x$ under the current model, or
\begin{equation}
    P(x_n) = w_n \sum_{k} N(x_n | \mu_k, \Sigma_k) \pi_k,
\end{equation}
where $N(x_n | \mu_k, \Sigma_k)$ is the multivariate Gaussian density,
\begin{equation}
    N(x | \mu, \Sigma) = {1 \over {(2\pi)^{D/2} \det(\Sigma)^{1/2}}}
    \exp[-{1 \over 2} (x - \mu) \cdot \Sigma^{-1} \cdot (x - \mu)].
\end{equation}
$P(x)$ can be split into the $k$ individual probabilities for each $x$:
\begin{equation}
    p_{nk} = {{N(x_n | \mu_k, \Sigma_k) \pi_k w_n} \over P(x_n)}
\end{equation}
The above equations describe the expectation step (E-step) of the EM
algorithm. The means, covariances, and mixture weights are estimated from
$p_{nk}$ in the maximization step (M-step) as follows.
\begin{equation}
    \mu_k = \sum_{n} p_{nk} x_n \Big/ \sum_{n} p_{nk}
\end{equation}
\begin{equation}
    \Sigma_k = \sum_{n} p_{nk} (x_n - \mu_k) (x_n - \mu_k) \Big/ \sum_{n} p_{nk}
\end{equation}
\begin{equation}
    \pi_k = {1 \over N} \sum_{n} p_{nk}
\end{equation}

The iterative EM algorithm is initilized by guessing initial values for the means,
covariances, and mixture weights. The initial means are randomly selected from the
training data (being careful to avoid repeats), the covarainces are each initialized
as the D-dimensional identity matrix, and the mixture weights are initialized as
$1/k$. An E-step is done to get a new $p_{nk}$ array, and then an M-step is done
to estimate new means, covariances, and weights. This is repeated until the
change in $\mathscr{L}$ is below a predetermined threshold.

\subsection{Iterative Update of the Gaussian Mixture Model}

The standard expectation-maximization algorithm has been modified to allow
iterative, online updates of the model by calling \texttt{model.update()} with
a new set of samples and weights. A new GMM is first trained using the new data.
Each component $k$ in the new model is then matched to a component $j$ in the old
model in such a way as to minimize the total Mahalanobis distance $D_M$ between
all of the means:
\begin{equation}
    \sum M_D = \sum_{j,k=0}^{K} (\sqrt{(\mu_j - \mu_k)^T\Sigma_k^{-1}(\mu_j - \mu_k)}
    + \sqrt{(\mu_k - \mu_j)^T\Sigma_j^{-1}(\mu_k - \mu_j)})
\end{equation}
The combination of components which minimizes $\sum M_D$ is the one that is kept. \\

Once the components have been matched, they are combined. Parameters of the old
model are denoted with a $j$, parameters of the new model are denoted with a
$k$, $N$ represents the total number of points the old model has been trained
on, $M$ is the number of samples in the new batch, and $\pi$ represents
component weights.
\begin{equation}
    \mu = {{N\pi_j\mu_j + M\pi_k\mu_k} \over {N\pi_j + M\pi_k}}
\end{equation}
\begin{equation}
    \Sigma = {{N\pi_j\Sigma_j + M\pi_k\Sigma_k} \over {N\pi_j + M\pi_k}}
    + {{N\pi_j\mu_j\mu_j^T + M\pi_k\mu_k\mu_k^T} \over {N\pi_j + M\pi_k}}
    - \mu\mu^T
\end{equation}
\begin{equation}
    \pi = {{N\pi_j + M\pi_k} \over {N + M}}
\end{equation}

\subsection{Scoring Samples}

The \texttt{score()} function takes an array of samples $x_n$ and returns an
array of probability densities. For samples drawn from an untruncated distribution
this is simply the sum of the multivariate Gaussian probability densities of each
point $x$ for each mixture component $k$,
\begin{equation}
    p(x_n) = \sum_{k} N(x_n | \mu_k, \Sigma_k),
\end{equation}
\begin{equation}
    N(x | \mu, \Sigma) = {1 \over {(2\pi)^{D/2} \det(\Sigma)^{1/2}}}
    \exp[-{1 \over 2} (x - \mu) \cdot \Sigma^{-1} \cdot (x - \mu)].
\end{equation}

If $x_n$ is drawn from a truncated distribution, however, $p(x_n)$ must be corrected
for this truncation (because the PDF of a distribution must integrate to 1).
An approximation of the necessary correction using a simple Monte Carlo method is
generally enough. \\

$n$ samples $x'$ are drawn from a full, untruncated distribution
according to the model's parameters. The correction constant $m$ is the
fraction of points in $x'$ that fall inside the provided bounds. This ratio can
be determined quickly using numpy's \texttt{less()}, \texttt{greater()}, and
\texttt{logical\_and()} functions. The scores given by $p(x_n)$ are then divided
by $m$.

\subsection{Sampling from the GMM}

The \texttt{sample()} function in \texttt{gaussian\_mixture\_model.py} generates
$N$ Gaussian samples according to the current model. For each component in $k$,
$n = \lfloor N\pi_k \rfloor$ samples are generated with mean $\mu_k$ and
covariance $\Sigma_k$. The final set of samples has $n$ such that the desired
number of samples $N$ is reached. \\

If the user does not specify bounds when calling the \texttt{sample()} function,
samples are generated using numpy's \texttt{multivariate\_normal()}. However,
the Monte Carlo integrator always specifies bounds, so the \texttt{sample()}
function from \texttt{multivariate\_truncnorm} is used instead.

\subsection{Preventing Non-Positive-Semidefinite Covariances}

In the training stage (particularly in the M-step or when merging two components
during an update), a covariance matrix can become non-positive-semidefinite.
Two methods are used to attempt to correct this. The first is to average a small
fraction of the previous covariance with the current one, or
\begin{equation}
    \Sigma = {{\Sigma_{i} + r \Sigma_{i-1}} \over {1 + r}}
\end{equation}
where $\Sigma$ is the final covariance, $\Sigma_i$ is the calculated covariance
for iteration $i$, $\Sigma_{i-1}$ is the previous covariance, and $r$ is a small
constant. By default $r = 0.1$. \\

The second, less preferable method is to reinitialize covariances that become
non-positive-semidefinite. Each iteration of the EM algorithm and each time two
covariances are merged the function \texttt{is\_pos\_def()} attempts to take the
Cholesky decomposition of the covariance. This returns an error if the covariance
is not positive-semidefinite, indicating that the covariance must be
reinitialized to the identity matrix. \\

Despite both of these methods, the tendency for covariances to become
non-positive-semidefinite in high dimensions or with large numbers of components
is a source of instability.

\section{Iterative Algorithm}

The algorithm's main function, \texttt{integrate()}, is:
\begin{verbatim}
def integrate(self, func, min_iter=10, max_iter=20, var_thresh=0.03):
    while self.iterations < max_iter:
        self.sample()
        self.value_array = func(self.sample_array)
        self.calculate_results()
        print(self.integral)
        self.iterations += 1
        if self.iterations >= min_iter and self.var < var_thresh:
            break
        self.train()
\end{verbatim}

\subsection{Sampling}

The first step in the algorithm is to sample the domain of integration.
If an array of prior samples and responsibilities is not provided by the user,
the initial prior is assumed to be uniform. The \texttt{sample()} function
populates the \texttt{sample\_array} and \texttt{p\_array} objects. \\

\texttt{sample\_array} is initialized as an empty $(N \times d)$ array, and
\texttt{p\_array} is initialized as a $(N \times 1)$ array of ones. For each
group of dimensions in \texttt{gmm\_dict}, if \texttt{gmm\_dict[dim\_group]} is
a GMM object, samples are generated using
\\ \texttt{gaussian\_mixture\_model.sample()}.
Each column in this new array of samples is placed in its corresponding column in
\texttt{sample\_array} (based on the dimensions in the current \texttt{dim\_group}).
The samples are scored using
\\ \texttt{gaussian\_mixture\_model.score()}, and
\texttt{p\_array} is multiplied by these scores. \\

If \texttt{gmm\_dict[dim\_group]} is \texttt{None}, that set of dimensions is
sampled uniformly. For left limits $a$ and right limits $b$, the scores for this
group of dimensions are
\begin{equation}
    p(x) = \prod_{d} {1 \over {b_d - a_d}},
\end{equation}
which are again multiplied with \texttt{p\_array}. These scores can be seen as
the inverse of the volume of the region from which the samples are taken. The
final responsibilities are
\begin{equation}
    p(x) = \prod_{q=0}^{Q} p_q(x),
\end{equation}
where each $q$ is a set of dimensions and $p_q(x)$ is the corresponding
responsibility of $x$ in that set of dimensions.

\subsection{Calculating Results}

The integrand $f(x)$ is evaluated over \texttt{sample\_array} and \texttt{value\_array}
is populated with the result. The current integral and variance are calculated
as
\begin{equation}
    I = {{1 \over N} \sum_{n=0}^{N} {f(x_n) \over {p(x_n)}}}.
\end{equation}
\begin{equation}
    \sigma^2 = {1 \over N^2}{\sum\limits_{n=0}^{N} {(g(x_n) - \langle g \rangle)}^2},
\end{equation}
where
\begin{equation}
    g(x) = {f(x) \over {p(x)}}
\end{equation}
and $\langle g \rangle$ is the average of $g(x)$ over $n$. \\

The total integral and variance are updated each iteration using a weighted average
over all iterations. The total integral is
\begin{equation}
    I_{total} = {{M I_{prev} + I} \over {M + 1}},
\end{equation}
where $M$ is the total number of previous iterations, and the total variance is
\begin{equation}
    \sigma^2_{total} = {{M \sigma^2_{prev} + \sigma^2} \over {M + 1}}.
\end{equation}

The "effective" number of samples, $n_{eff}$, is also updated each iteration. Let
$G(x)$ be the cumulative history of $g(x)$ over all iterations, then
\begin{equation}
    n_{eff} = {1 \over \max{G(x)}} \sum_{n=0}^{N (M+1)} G(x_n).
\end{equation}

\subsection{Training}

For each group of dimensions in \texttt{gmm\_dict}, the corresponding samples
are placed in a temporary array. If \texttt{gmm\_dict[dim\_group]} is
\texttt{None}, a new GMM oject is initialized. The model is then fit using
\texttt{model.fit()}. If \texttt{gmm\_dict[dim\_group]} is a GMM object,
an update is done using \texttt{model.update()}. In either case, the sample
weights are
\begin{equation}
    w(x) = \left| {f(x) \over p(x)} \right|.
\end{equation}

\end{document}
