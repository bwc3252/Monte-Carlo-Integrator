\documentclass{article}
\usepackage{mathtools}
\newcommand{\vect}[1]{\boldsymbol{#1}}

\title{Monte Carlo Integrator}
\date{July 15, 2018}
\author{Ben Champion \\ bwc3252@rit.edu}
\begin{document}
\maketitle

\section{Basic Algorithm}

The integrand $f(x)$ is sampled over a domain $V$ according to a distribution
$p(x)$. The Monte Carlo integral over $V$ is
\begin{equation}
    I = {{V \over N} \sum_{n=0}^{N} {f(x_n) \over {p(x_n)}}}.
\end{equation}
The variance estimate for this integral is given by
\begin{equation}
    Var = {1 \over N^2}{\sum\limits_{n=0}^{N} {(g(x_n) - \langle g \rangle)}^2},
\end{equation}
where
\begin{equation}
    g(x_n) = {f(x_n) \over {p(x_n)}}
\end{equation}
and $\langle g \rangle$ is the average of $g(x_n)$ over $n$. \\

In order to reduce this variance without increasing the number of points
sampled, the distribution $p(x)$ is chosen to approximate $f(x)$. This is done
by iteratively sampling from a Gaussian Mixture Model (GMM), integrating using
these samples, and retraining the GMM to better approximate the integrand,
stopping either at a predetermined number of iterations or when the variance
falls below a certain threshold. When training the GMM each sample is weighted
by the ratio of its function value to its responsibility under the current
distribution, or
\begin{equation}
    w(x_n) = \left| {f(x_n) \over p(x_n)} \right|.
\end{equation}
Weighting the samples in this way means that the integrator will converge to
the correct distribution regardless of how the first iteration is sampled. In
practice, the first iteration is sampled uniformly.

\section{Initialization}

Whether using \texttt{monte\_carlo\_integrator.py} directly or within
\texttt{mcsampler\_new.py}, the integrator is initialized with the following
parameters:

\begin{enumerate}

\item \texttt{d}: Number of dimensions of the integrand's domain

\item \texttt{bounds}: \texttt{(d $\times$ 2)} array where each row is
\texttt{[left\_limit, right\_limit]} for its corresponding dimension

\item \texttt{gmm\_dict}: Python dictionary where each key is a tuple of
dimensions and each value is either a sklearn gmm object or \texttt{None}

\item \texttt{n\_comp}: Number of Gaussian components per mixture model

\end{enumerate}
The integrator is called using \texttt{monte\_carlo\_integrator.integrate(func)},
where \texttt{func} is a numpy vectorized function. The \texttt{err\_thresh} and
\texttt{max\_count} parameters can also be specified when calling the integrator.

\section{Gaussian Mixture Model}

\subsection{Basic Gaussian Mixture Model Implementation}

The Gaussian mixture model implemented in \texttt{gaussian\_mixture\_model.py}
fits a mixture of \texttt{k} Gaussians to a set of weighted data using the
expectation-maximization (EM) algorithm.

\section{Training the GMM}

The function \texttt{fit\_gmm()}
fits a Gaussian Mixture Model (GMM) to each group of dimensions specified by
\texttt{gmm\_dict}. Note that in training the GMM, samples drawn from a full,
untruncated sampling distribution are used.\\

The weights used to fit the mixture model are given by

\begin{equation}
    weights = \left|value\_array \over p\_array\right|
\end{equation}
where \texttt{value\_array} is a vertical array of function values for each
point, and \texttt{p\_array} is a vertical array of the responsibility of each
point under the current sampling distribution. \\

For each set of dimensions in \texttt{gmm\_dict}, the samples for that set of
dimensions are placed in a temporary array, and if a corresponding sklearn gmm
object does not already exist (i.e. \texttt{gmm\_dict[dim\_group] == None}), one
is initialized. The GMM is then trained using
\texttt{fit(X=samples\_to\_fit, w=weights)}, and \texttt{gmm\_dict} is updated
with the new GMM. \\

If sklearn is unable to fit a GMM to the data, \texttt{gmm\_dict} is updated
with \texttt{None}.

\section{Sampling}

\subsection{Truncated Multivariate Gaussian Samples}

The file \texttt{multivariate\_truncnorm.py} provides a \texttt{sample()}
function to draw samples from a multivariate normal distribution inside a set of
d-dimensional rectangular bounds. The parameters of \texttt{sample()} are the
mean and covariance of the distribution, the bounds of the sampling region, and
the number of samples to draw. \\

In general, multivariate Gaussian samples can be generated by sampling from a
Gaussian that is centered at the origin with a covariance of 1, applying a
linear transformation determined from the covariance matrix, and shifting the
resulting samples so that the mean falls in the correct place. For a covariance
matrix $\Sigma$ and mean $\mu$, the final samples are generated as

\begin{equation}
    x' = \lambda^{1/2} \phi x + \mu,
\end{equation}
where $x'$ is the final sample, $\lambda$ is diagonal matrix of the eigenvalues
of $\Sigma$, $\phi$ is a matrix of the eigenvectors of $\Sigma$, and $x$ is the
original "base sample." \\

Truncated samples are generated through a similar process. Scipy's
\texttt{truncnorm} can be used to generate univariate truncated Gaussian
samples. Univariate samples with covariance 1 and mean 0 are used to create an
array of multivariate samples centered at the origin with covariance 1. The
above transformation (equation 5) is used to create truncated samples from the
desired distribution. \\

The problem with this approach, however, is that the bounds are transformed
along with the points. This means that the samples fill a d-dimensional
parallelogram-shaped region rather than a d-dimensional rectangular one. This
problem is remedied by drawing the initial samples from a slightly larger region
than is required and rejecting the samples that fall outside the bounds after
being transformed. \\

These new bounds are determined using the inverse of the transformation, or

\begin{equation}
    r = [\lambda^{1/2} \phi]^{-1}.
\end{equation}

Each corner point of the desired sampling region is transformed by $r$ to find
the new, transformed corner points.  The minimum and maximum coordinates for
each dimension describe the smallest rectangular region that, when transformed
using equation 5, will contain the desired region. \\

Unlike traditional rejection sampling, where points are sampled from the entire
distribution and then truncated in the end, this method will not suffer drops in
efficiency based on where the mean of the distribution is in relation to the
bounds. Instead, the limiting factor is the geometry of the transformation.

\subsection{Sampling from the GMM}

The function \texttt{sample\_from\_gmm()} takes \texttt{gmm\_dict} and the
integrand \texttt{func} as parameters. Two empty arrays,
\texttt{sample\_array} (for untruncated samples) and \texttt{sample\_array\_i}
(for truncated samples) are initialized. Two arrays of ones, \texttt{p\_array}
and \texttt{p\_array\_i}, are initialized to hold the corresponding sample
responsibilities. For each group of dimensions in \texttt{gmm\_dict}, if
\texttt{gmm\_dict[dim\_group]} is a sklearn gmm object, that group of dimensions
 is sampled as follows: \\

The lists of means, covariances, and component weights are retrieved from the
gmm. For each component, the number of samples to draw is

\begin{equation}
    n = N \cdot w,
\end{equation}
where $N$ is the total number of samples to draw and $w$ is the weight of the
component. Using the corresponding mean and covariance matrix, $n$ samples are
drawn from an untruncated distribution using numpy's
\texttt{multivariate\_normal} and put in \texttt{sample\_array}, and
$n$ samples are drawn from a truncated distribution using
\texttt{multivariate\_truncnorm} and put in \texttt{sample\_array\_i}. This is
done for each component in the current gmm. \\

The partial responsibilities for both sets of samples are found using sklearn's
\texttt{score()} function. These responsibilities are multiplied by
\texttt{p\_array} or \texttt{p\_array\_i}, depending on which array of samples
they correspond with. \\

This is repeated for every set of dimensions in \texttt{gmm\_dict}. The final
responsibility for each sample $x$ is

\begin{equation}
    p(x) = \prod_{m=0}^{M} p(x_m),
\end{equation}
where $p(x_m)$ is the responsibility of the group of dimensions $m$, and $M$ is
the total number of groups of dimensions. \\

If \texttt{gmm\_dict[dim\_group]} has no sklearn gmm object, it is sampled
uniformly, and the responsibilities are updated as

\begin{equation}
    p(x) = p_{prev.}(x) \prod_{d=0}^{D} {1 \over {rlim_d - llim_d}},
\end{equation}
where $D$ is the total number of dimensions in the group of dimensions, and
$rlim_d$ and $llim_d$ describe the left and right limits of dimension d,
accordingly. This essentially amounds to dividing $p(x)$ by the D-dimensional
volume of the region the samples are taken from.

\section{Integration}

$p(x)$ is the PDF of the current sampling distribution. For an array of samples
from $p(x)$ \texttt{sample\_array}, \texttt{k} is the fraction of samples in
\texttt{sample\_array} that fall inside the limits of integration.
\texttt{sample\_array\_i} is an array of samples from $p(x)$ that is truncated
at the the limits of integration, and \texttt{p\_array\_i} is an array of the
responsibility of each point in \texttt{sample\_array\_i} according to $p(x)$.
\texttt{value\_array\_i} is the function value at each point in
\texttt{sample\_array\_i}. The integral \texttt{I} is then

\begin{equation}
    I ={ k \over N} \sum {value\_array\_i \over p\_array\_i}.
\end{equation}

The ratio \texttt{k} is necessary because $p(x)$ is only normalized to 1 on an
infinite domain. The points in \texttt{sample\_array\_i} are sampled from a
finite domain, so their responsibilities \texttt{p\_array\_i} must be adjusted
by a constant factor \texttt{k} to renormalize them.

\section{Iterative Algorithm}

On the first iteration the integrand is sampled uniformly and \texttt{p\_array}
is an array of ones. This means that the training weights are the function
values. The GMM is trained using these samples, and new samples are drawn according
to it. The integral and error are calculated (the error is the square root of
the variance). The GMM is retrained using the new samples and function values,
and this process is iteratively repeated until one or more of the stopping
conditions is met. \\

After calculating the integral and error, if the error is small enough relative
to the size of the integral, the integral and variance are added to lists which
keep track of each iteration. The running variance is calculated as the weighted
average of the variance for each iteration, and the running error is the square
root of this variance. The weights are the normalized sums of the
responsibilities of the samples for each iteration. \\

The algorithm stops when either the desired error threshold is met or the
maximum number of iterations is reached. To avoid the integrator stopping too
early, it will only stop after a minimum number of iterations is reached,
regardless of the running error. The final integral is the weighted average
of each individual integral, where the weights are once again the sums of the
sample responsibilities for each iteration.

\end{document}
